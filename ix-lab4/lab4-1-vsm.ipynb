{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *J*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Ann-Kristin Bergmann*\n",
    "* *Nephele Aesopou*\n",
    "* *Ewa Miazga*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n",
      "[nltk_data] Downloading package punkt to /home/aesopou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aesopou/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl, save_json\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from scipy import linalg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The latest developments in processing and the novel generations of organic composites are discussed. Nanocomposites, adaptive composites and biocomposites are presented. Product development, cost analysis and study of new markets are practiced in team work. Content Basics of composite materialsConstituentsProcessing of compositesDesign of composite structures\\xa0Current developmentNanocomposites Textile compositesBiocompositesAdaptive composites\\xa0ApplicationsDriving forces and marketsCost analysisAerospaceAutomotiveSport Keywords Composites - Applications - Nanocomposites - Biocomposites - Adaptive composites - Design - Cost Learning Prerequisites Required courses Notion of polymers Recommended courses Polymer Composites Learning Outcomes By the end of the course, the student must be able to: Propose suitable design, production and performance criteria for the production of a composite partApply the basic equations for process and mechanical properties modelling for composite materialsDiscuss the main types of composite applications Transversal skills Use a work methodology appropriate to the task.Use both general and domain specific IT resources and toolsCommunicate effectively with professionals from other disciplines.Evaluate one's own performance in the team, receive and respond appropriately to feedback. Teaching methods Ex cathedra and invited speakers Group sessions with exercises or work on the project Expected student activities Attendance at lectures Design of a composite part, bibliography search \\xa0 Assessment methods Written exam report and oral presentation in class\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses[0]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of courses in our dataset = 854\n"
     ]
    }
   ],
   "source": [
    "num_courses = len(courses)\n",
    "print(\"Number of courses in our dataset =\", num_courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'MSE-440')\n"
     ]
    }
   ],
   "source": [
    "# Keep track of courseId and the course's index\n",
    "\n",
    "# (index, courseId)\n",
    "courseId_index = []\n",
    "for i, course in enumerate(courses):\n",
    "    courseId_index.append((i, course['courseId']))\n",
    "# get an example\n",
    "print(courseId_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(text):\n",
    "    for i in range(len(text)):\n",
    "        \n",
    "        # tokenize\n",
    "        text[i]['description'] = word_tokenize(text[i]['description'], language= 'english')\n",
    "                \n",
    "        # lowercase\n",
    "        text[i]['description'] = [w.lower() for w in text[i]['description']]\n",
    "        \n",
    "        # remove stopwords\n",
    "        text[i]['description'] = [w for w in text[i]['description'] if not w in stopwords]\n",
    "        \n",
    "        # Lemming\n",
    "        text[i]['description'] = [lemmatizer.lemmatize(w) for w in text[i]['description'] if w.isalpha()]\n",
    "        \n",
    "        # Stemming on words only (remove numbers and urls)\n",
    "        #text[i]['description'] = [ps.stem(w) for w in text[i]['description'] if w.isalpha()]\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text[i]['description'] = [t for t in text[i]['description'] if not t in string.punctuation]   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, remove very short word-strings because there are no usefull words with two or less letters...\n",
    "for course in courses:\n",
    "    course['description'] = [item for item in course['description'] if len(item) > 2]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that gets a text and creates n-grams of the tokens. \n",
    "ngrams=[]\n",
    "def n_grams(text, n):\n",
    "    for i in range(len(text)):\n",
    "        ngrams_curr = zip(*[text[i]['description'][j:] for j in range(n)])\n",
    "        ngrams.append([\" \".join(ngram) for ngram in ngrams_curr])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = n_grams(courses, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bigrams to the vocabulary\n",
    "for i in range(num_courses):\n",
    "    courses[i]['description'] += bigrams[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function crates a word_corpus by adding the words from all documents to one list.\n",
    "# Word_corpus contains also duplicates because we need them to count the occurences of words.\n",
    "def make_word_corpus(text):\n",
    "    word_corpus = []\n",
    "    for course in courses:\n",
    "        for token in course['description']:\n",
    "            word_corpus.append(token)\n",
    "    # Sort the words alphabetically\n",
    "    word_corpus.sort()\n",
    "    \n",
    "    return word_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name it as word_corpus\n",
    "word_corpus = make_word_corpus(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word_corpus contains 258378 words.\n",
      "There are 90319 unique words in our corpus.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Word_corpus contains {len(word_corpus)} words.\")\n",
    "print(f\"There are {len(np.unique(word_corpus))} unique words in our corpus.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Explain which ones you implemented and why.**\n",
    "\n",
    "First of all, we tokenize the course description, which means that we split the text into words. Then, we make all words lowercase so that we do not have duplicates and remove the stopwords using the stopwords file given. The stopwords are removed because they do not are of importance to the description.\n",
    "\n",
    "We then tried both lemming and stemming and we decided that the best way for this text corpus was lemming. Stemming reduces the word to its \"root\" while lemming considers the word's meaning and words with the same root have not necessarily the same meaning. In our courses context this is important.\n",
    "\n",
    "Finally, we remove punctuation using the feature string.punctuation. Another decision that was made was to remove words with 2 or less letters because they are mostly with no meaning. We decided to also add bigrams to our bag of words because most of the course descriptions use them. There is no use for 3-grams or more because very few course terms are like that, and it would makde our word corpus very heavy.\n",
    "\n",
    "In 4.2, we also remove words with low frequency. We chose the low frequency in this case to be 2 or less because some tokens may belong to only one course, so our boundary frequency cannot be too high. This of course can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the index of a course in the courses list, if given the courseId.\n",
    "def find_index(courseId):\n",
    "    index = None\n",
    "    for i in range(num_courses):\n",
    "        if courseId_index[i][1] == courseId:\n",
    "            index = courseId_index[i][0]\n",
    "            break\n",
    "\n",
    "    return index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Print the terms in the pre-processed description of the IX class in alphabetical order.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words tokens for course COM-308:\n",
      "\n",
      "['acquired', 'acquired lecture', 'activity', 'activity lecture', 'advertisement', 'advertisement class', 'algebra', 'algebra', 'algebra algorithm', 'algebra markov', 'algorithm', 'algorithm', 'algorithm data', 'algorithm statistic', 'analysis', 'analysis user', 'analytics', 'analytics', 'analytics application', 'analytics collection', 'apache', 'apache spark', 'application', 'application', 'application inspired', 'application social', 'assessment', 'assessment method', 'auction', 'auction', 'auction learning', 'auction provide', 'balance', 'balance foundational', 'based', 'based', 'based hadoop', 'based number', 'basic', 'basic', 'basic', 'basic linear', 'basic material', 'basic model', 'cathedra', 'cathedra homework', 'chain', 'chain java', 'class', 'class', 'class', 'class explores', 'class lab', 'class seek', 'cloud', 'cloud service', 'clustering', 'clustering', 'clustering community', 'clustering community', 'collection', 'collection modeling', 'combination', 'combination theoretical', 'communication', 'communication recommended', 'community', 'community', 'community detection', 'community detection', 'computing', 'computing', 'computing auction', 'computing online', 'concept', 'concept', 'concept lab', 'concept start', 'concrete', 'concrete problem', 'content', 'content class', 'course', 'course', 'course basic', 'course stochastic', 'coverage', 'coverage main', 'curated', 'curated class', 'current', 'current practice', 'data', 'data', 'data', 'data', 'data', 'data', 'data mining', 'data mining', 'data mining', 'data online', 'data online', 'data structure', 'datasets', 'datasets', 'datasets curated', 'datasets real', 'decade', 'decade content', 'dedicated', 'dedicated infrastructure', 'designed', 'designed explore', 'detection', 'detection', 'detection model', 'detection topic', 'dimensionality', 'dimensionality reduction', 'draw', 'draw knowledge', 'effectiveness', 'effectiveness machine', 'efficiency', 'efficiency effectiveness', 'end', 'end student', 'exam', 'expected', 'expected student', 'explore', 'explore', 'explore', 'explore', 'explore basic', 'explore data', 'explore datasets', 'explore practical', 'explores', 'explores number', 'field', 'field application', 'final', 'final exam', 'foundational', 'foundational basic', 'framework', 'framework model', 'function', 'function online', 'fundamental', 'fundamental concept', 'good', 'good coverage', 'graph', 'graph', 'graph linear', 'graph theory', 'hadoop', 'hadoop', 'hadoop apache', 'hadoop recommender', 'homework', 'homework', 'homework explore', 'homework lab', 'important', 'important concept', 'information', 'information', 'information network', 'information retrieval', 'infrastructure', 'infrastructure based', 'inspired', 'inspired current', 'internet', 'internet', 'internet analytics', 'internet cloud', 'java', 'java learning', 'key', 'key function', 'keywords', 'keywords data', 'knowledge', 'knowledge acquired', 'lab', 'lab', 'lab', 'lab designed', 'lab draw', 'lab session', 'laboratory', 'laboratory session', 'learning', 'learning', 'learning', 'learning', 'learning outcome', 'learning prerequisite', 'learning social', 'learning technique', 'lecture', 'lecture', 'lecture assessment', 'lecture homework', 'linear', 'linear', 'linear algebra', 'linear algebra', 'machine', 'machine', 'machine learning', 'machine learning', 'main', 'main data', 'markov', 'markov chain', 'material', 'material', 'material algorithm', 'material weekly', 'medium', 'medium combination', 'method', 'method', 'method cathedra', 'method project', 'midterm', 'midterm final', 'mining', 'mining', 'mining', 'mining analytics', 'mining machine', 'mining problem', 'model', 'model', 'model', 'model', 'model', 'model communication', 'model dimensionality', 'model fundamental', 'model information', 'model typical', 'modeling', 'modeling analysis', 'network', 'network recommender', 'networking', 'networking', 'networking', 'networking hadoop', 'networking search', 'networking social', 'number', 'number', 'number datasets', 'number key', 'online', 'online', 'online', 'online', 'online', 'online auction', 'online service', 'online service', 'online servicesanalyze', 'online servicesdevelop', 'outcome', 'outcome end', 'past', 'past decade', 'practical', 'practical question', 'practice', 'practice internet', 'prerequisite', 'prerequisite required', 'problem', 'problem', 'problem online', 'problem teaching', 'project', 'project midterm', 'provide', 'provide good', 'question', 'question based', 'real', 'real world', 'recommended', 'recommended course', 'recommender', 'recommender', 'recommender system', 'recommender system', 'reduction', 'reduction stream', 'related', 'related field', 'required', 'required course', 'retrieval', 'retrieval stream', 'search', 'search advertisement', 'seek', 'seek balance', 'service', 'service', 'service', 'service social', 'service specifically', 'service ubiquitous', 'servicesanalyze', 'servicesanalyze efficiency', 'servicesdevelop', 'servicesdevelop framework', 'session', 'session', 'session expected', 'session explore', 'social', 'social', 'social', 'social', 'social', 'social information', 'social medium', 'social networking', 'social networking', 'social networking', 'spark', 'spark keywords', 'specifically', 'specifically social', 'start', 'start graph', 'statistic', 'statistic graph', 'stochastic', 'stochastic model', 'stream', 'stream', 'stream computing', 'stream computing', 'structure', 'structure important', 'student', 'student', 'student activity', 'student explore', 'system', 'system', 'system clustering', 'system clustering', 'teaching', 'teaching method', 'technique', 'technique concrete', 'theoretical', 'theoretical material', 'theory', 'theory related', 'topic', 'topic model', 'typical', 'typical data', 'ubiquitous', 'ubiquitous past', 'user', 'user data', 'weekly', 'weekly laboratory', 'work', 'work dedicated', 'world', 'world work']\n"
     ]
    }
   ],
   "source": [
    "COM_308_tokens = courses[find_index('COM-308')]['description']\n",
    "COM_308_tokens.sort()\n",
    "print(\"Words tokens for course COM-308:\\n\")\n",
    "print(COM_308_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function gets a word corpus and an empty dictionary and creates a (key, value) = (word, # of occurrences). \n",
    "def get_word_count(corpus, dictionary):\n",
    "    for word in corpus:\n",
    "        dictionary[word] += 1\n",
    "#   return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a dict where each potential key is an int\n",
    "word_corpus_count = defaultdict(int)\n",
    "\n",
    "# Build a dictionary called word_corpus_count\n",
    "get_word_count(word_corpus, word_corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90319\n"
     ]
    }
   ],
   "source": [
    "print(len(word_corpus_count)) # This number agrees with our unique word corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low frequency words --set low frew\n",
    "def remove_low_freq(corpus, n):\n",
    "    # Find the words with frequency in the corpus less than n\n",
    "    low_freq = [key for key, value in corpus.items() if value < n]\n",
    "\n",
    "    # Remove them from the description of the courses they belong to\n",
    "    for course in courses:\n",
    "        course['description'] = [word for word in course['description'] if word not in low_freq]\n",
    "    \n",
    "    # Build a new corpus with the higher-frequency terms\n",
    "    corpus = {key: value for key, value in corpus.items() if value >= n}\n",
    "    \n",
    "    # return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_low_freq(word_corpus_count, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When we remove low frequency terms we have:\n",
      "There are 90319 unique words in our corpus.\n"
     ]
    }
   ],
   "source": [
    "print(\"When we remove low frequency terms we have:\")\n",
    "print(f\"There are {len(np.unique(word_corpus))} unique words in our corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_corpus = make_word_corpus(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To access each document seperately, make a list of dictionaries with (key,value) = (word in doc, frequency in doc)\n",
    "bag_docs_words = []\n",
    "for j in range(num_courses):\n",
    "    diction = defaultdict(int)\n",
    "    for word in courses[j]['description']:\n",
    "        diction[word] += 1\n",
    "        \n",
    "    bag_docs_words.append(diction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a list containing lists of the tokens of each course\n",
    "course_tokens =[set(course.keys()) for course in bag_docs_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First find the max frequency among the words in each document\n",
    "max_word_in_doc = {i : max(dic.values()) for i,dic in enumerate(bag_docs_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Inverse Document frequency ---> diminishes the weight of terms that occur very frequently in the document set and\n",
    "# increases the weight of terms that occur rarely.\n",
    "idf = {w: -math.log2(sum([1 for course in course_tokens if w in course]) / num_courses) for w in word_corpus}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF_IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a unique word corpus\n",
    "unique_word_corpus = list(np.unique(np.array(word_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix_values is a list containing all the TF_IDF values that we are going to insert in the matrix\n",
    "matrix_values = []\n",
    "\n",
    "# Val_position is a list containing tuples of (word index, document index) for each of the above values.\n",
    "val_position = []\n",
    "\n",
    "n = 0\n",
    "for course in bag_docs_words:\n",
    "    \n",
    "    # Get the word indexes in unique_word_corpus of each word in the document \n",
    "    word_pos = [unique_word_corpus.index(key) for key in list(course.keys())]\n",
    "    words = len(word_pos) # num of words in the current document\n",
    "    \n",
    "    # Get the IDF value for each of the words\n",
    "    curr_idf = [idf[word] for word in list(course.keys())]\n",
    "    \n",
    "    # Calculate TF for this document\n",
    "    TF = [val/ max_word_in_doc[n] for val in list(course.values())]\n",
    "    \n",
    "    # Multiply TF * IDF\n",
    "    values = [x*y for x,y in zip(TF, curr_idf)]\n",
    "    \n",
    "    matrix_values.extend(values)\n",
    "    \n",
    "    for j in range(words):\n",
    "        val_position.append((word_pos[j], n))\n",
    "        \n",
    "    n+=1\n",
    "    \n",
    "# Change into an array\n",
    "positions = np.array(val_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TFIDF matrix\n",
    "\n",
    "# Define the size of the matrix\n",
    "m = len(unique_word_corpus)\n",
    "n = num_courses\n",
    "\n",
    "# Set the values = TFIDF\n",
    "values = matrix_values\n",
    "\n",
    "# Define their position in the matrix\n",
    "rows = positions[:,0]\n",
    "columns = positions[:,1]\n",
    "\n",
    "X = csr_matrix((matrix_values, (rows, columns)), shape=(m, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10820, 854)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz, load_npz\n",
    "save_npz('X_matrix.npz', X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Print the 15 terms in the description of the IX class with the highest TF-IDF scores.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "IX_class = find_index('COM-308')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['social networking', 'online', 'social', 'data mining', 'explore', 'mining', 'networking', 'community detection', 'hadoop', 'recommender', 'recommender system', 'service', 'auction', 'datasets', 'internet']\n"
     ]
    }
   ],
   "source": [
    "# Accessing column of the given class\n",
    "column_IX = X[:, IX_class]  # Remember, index starts from 0\n",
    "\n",
    "\n",
    "# Convert column to an array and get the values\n",
    "values = column_IX.toarray().flatten()\n",
    "# Getting the indices of the highest 15 values (- makes the sorting in descending order)\n",
    "top_15_indices = np.argsort(-values)[:15] \n",
    "\n",
    "# Getting the words\n",
    "top_15_words = [unique_word_corpus[i] for i in top_15_indices]\n",
    "print(top_15_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Explain where the difference between the large scores and the small ones comes from.**\n",
    "\n",
    "The large scores in the TF-IDF matrix mean that the specific word is very frequent in the specific document. However, it also means that it is not as frequent in all the rest documents.\n",
    "\n",
    "Small values in the matrix mean that the specific word is common to many documents and is not specific to the current one."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Display the top five courses together with their similarity score for each query.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity of two documents\n",
    "def similarity(doc1, doc2):\n",
    "    value = (doc1.T @ doc2) / (linalg.norm(doc1.toarray()) * linalg.norm(doc2.toarray()))\n",
    "    return value[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index of markov chain bigram: 5649\n",
      "  (0, 43)\t1.1551228895938142\n",
      "  (0, 80)\t2.7722949350251547\n",
      "  (0, 245)\t4.620491558375257\n",
      "  (0, 398)\t3.465368668781443\n",
      "  (0, 412)\t1.1551228895938142\n",
      "  (0, 417)\t1.2601340613750702\n",
      "  (0, 555)\t0.7700819263958761\n"
     ]
    }
   ],
   "source": [
    "# markov chain\n",
    "index_markov = unique_word_corpus.index('markov chains')\n",
    "print(\"index of markov chain bigram:\",index_markov)\n",
    "\n",
    "print(X[index_markov])\n",
    "\n",
    "# Convert to an array and perform argsort. [::-1] reverses the order of the indices obtained from np.argsort() --> descending\n",
    "# [:5] selects the first 5 indices from the reversed order.\n",
    "top5_markov = np.argsort(X[index_markov].toarray()[0])[::-1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markov Chain:\n",
      "\n",
      "\"Markov chains and algorithmic applications\" & \"Applied probability & stochastic processes\": score =  0.29414406139161475.\n",
      "\"Markov chains and algorithmic applications\" & \"Applied stochastic processes\": score =  0.3197696157233014.\n",
      "\"Markov chains and algorithmic applications\" & \"Stochastic calculus I\": score =  0.19203475147034796.\n",
      "\"Markov chains and algorithmic applications\" & \"Internet analytics\": score =  0.10838044263157404.\n",
      "\"Applied probability & stochastic processes\" & \"Applied stochastic processes\": score =  0.3140109171696444.\n",
      "\"Applied probability & stochastic processes\" & \"Stochastic calculus I\": score =  0.15509394061628803.\n",
      "\"Applied probability & stochastic processes\" & \"Internet analytics\": score =  0.07454353919365653.\n",
      "\"Applied stochastic processes\" & \"Stochastic calculus I\": score =  0.17457143406429212.\n",
      "\"Applied stochastic processes\" & \"Internet analytics\": score =  0.07449246262450858.\n",
      "\"Stochastic calculus I\" & \"Internet analytics\": score =  0.032650351312745.\n"
     ]
    }
   ],
   "source": [
    "print(\"Markov Chain:\\n\" )\n",
    "\n",
    "for i in range(5):\n",
    "    doc1 = X[:,top5_markov[i]] \n",
    "    course1 = courses[top5_markov[i]]['name']\n",
    "    \n",
    "    for j in range(i+1,5):\n",
    "        doc2 = X[:,top5_markov[j]] \n",
    "        course2 = courses[top5_markov[j]]['name']\n",
    "        \n",
    "        similarity_score = similarity(doc1,doc2)\n",
    "        print(f'\"{course1}\" & \"{course2}\": score =  {similarity_score}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index of facebook word: 3604\n",
      "  (0, 798)\t1.5375935146769195\n"
     ]
    }
   ],
   "source": [
    "index_facebook = unique_word_corpus.index('facebook')\n",
    "print(\"index of facebook word:\", index_facebook)\n",
    "print(X[index_facebook])\n",
    "\n",
    "# Facebook appears in only one course\n",
    "# Find its similarity score with the Markov chain courses\n",
    "\n",
    "# Get the column=course of the X matrix where the facebook word appears\n",
    "course_facebook = int(X[index_facebook].indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facebook & Markov chain:\n",
      "\n",
      "\"Markov chains and algorithmic applications\" & \"Computational Social Media\": score =  0.030149529921798212.\n",
      "\"Applied probability & stochastic processes\" & \"Computational Social Media\": score =  0.01170559340863037.\n",
      "\"Applied stochastic processes\" & \"Computational Social Media\": score =  0.014758550536404303.\n",
      "\"Stochastic calculus I\" & \"Computational Social Media\": score =  0.004213235867044565.\n",
      "\"Internet analytics\" & \"Computational Social Media\": score =  0.19592516704149104.\n"
     ]
    }
   ],
   "source": [
    "print(\"Facebook & Markov chain:\\n\" )\n",
    "\n",
    "for i in range(5):\n",
    "    doc1 = X[:,top5_markov[i]] \n",
    "    course1 = courses[top5_markov[i]]['name']\n",
    "    \n",
    "    doc2 = X[:, course_facebook] \n",
    "    course2 = courses[course_facebook]['name']\n",
    "        \n",
    "    similarity_score = similarity(doc1,doc2)\n",
    "    print(f'\"{course1}\" & \"{course2}\": score =  {similarity_score}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you think of the results? Give your intuition on what is happening**\n",
    "\n",
    "In general, we are getting higher similarity scores between the courses in markov chains than when comparing them with the facebook courses. The course that contains the bigram \"markov chains\" in its title has the highest scores with all courses. However, there are some scores which are very low, ie < 0.09. Hence, even if these courses are the top 5 containing \"markov chains\", they are not similar in most of their other words and get a low similarity.\n",
    "\n",
    "The highest score of the facebook course, Computational Social Media, is with the course Internet Analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 4: Word2Vec\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *J*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Ann-Kristin Bergmann*\n",
    "* *Nephele Aesopou*\n",
    "* *Ewa Miazga*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 4 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from utils import *\n",
    "import gensim\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aesopou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aesopou/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Specify the path to your vector file\n",
    "vector_file = '/ix/model.txt'\n",
    "\n",
    "# Load the vectors using KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(vector_file, binary=False, no_header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the loaded vectors\n",
    "word = 'apple'\n",
    "\n",
    "# Check if the word is in the vocabulary\n",
    "if word in model.key_to_index:\n",
    "    # Retrieve the vector representation of the word\n",
    "    vector = model.get_vector(word)\n",
    "else:\n",
    "    print(f\"'{word}' is not in the vocabulary.\")\n",
    "    \n",
    "vector_dim = len(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(text):\n",
    "    for i in range(len(text)):\n",
    "        \n",
    "        # tokenize\n",
    "        text[i]['description'] = word_tokenize(text[i]['description'], language= 'english')\n",
    "        \n",
    "        # take only words\n",
    "        text[i]['description'] = [w for w in text[i]['description'] if w.isalpha()]\n",
    "                 \n",
    "        # remove stopwords\n",
    "        text[i]['description'] = [w for w in text[i]['description'] if not w in stopwords]\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text[i]['description'] = [t for t in text[i]['description'] if not t in string.punctuation]   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing(courses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You would also need to make a reasonable choice for a ‘default’ vector for those words that\n",
    "#are in your dataset but are not present in the pre-trained model\n",
    "\n",
    "# Add n-grams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, remove very short word-strings because there are no usefull words with two or less letters...\n",
    "for course in courses:\n",
    "    course['description'] = [item for item in course['description'] if len(item) > 2]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function crates a word_corpus by adding the words from all documents to one list.\n",
    "# Word_corpus contains also duplicates because we need them to count the occurences of words.\n",
    "def make_word_corpus(text):\n",
    "    word_corpus = []\n",
    "    for course in courses:\n",
    "        for token in course['description']:\n",
    "            word_corpus.append(token)\n",
    "    # Sort the words alphabetically\n",
    "    word_corpus.sort()\n",
    "    \n",
    "    return word_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name it as word_corpus\n",
    "word_corpus = make_word_corpus(courses)\n",
    "\n",
    "# Make unique word corpus\n",
    "unique_word_corpus = list(np.unique(np.array(word_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word_corpus contains 133227 words.\n",
      "There are 17122 unique words in our corpus.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Word_corpus contains {len(word_corpus)} words.\")\n",
    "print(f\"There are {len(unique_word_corpus)} unique words in our corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.12 : Clustering word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vec(vector): \n",
    "    array = np.array(vector)\n",
    "    # Calculate the normalization factor\n",
    "    norm_factor = np.linalg.norm(array)\n",
    "    # Normalize the array\n",
    "    normalized_array = array / norm_factor\n",
    "    # Convert the normalized array back to a list (if needed)\n",
    "    normalized_data = normalized_array.tolist()\n",
    "    return normalized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 12260 words that belong to the pre-trained model out of 17122\n"
     ]
    }
   ],
   "source": [
    "# Build matrix word_vec_matrix\n",
    "num_words = len(unique_word_corpus)\n",
    "word_vec_matrix = []\n",
    "word_count = 0\n",
    "\n",
    "for word in unique_word_corpus:\n",
    "    # Check if the word is in the vocabulary\n",
    "    if word in model.key_to_index:\n",
    "        \n",
    "    # Retrieve the vector representation of the word\n",
    "        vector = model.get_vector(word)\n",
    "        word_vec_matrix.append(normalize_vec(vector))\n",
    "        \n",
    "        word_count +=1\n",
    "    else:\n",
    "        # here we decide that if the word is not in the given vocab, make it a zero vector\n",
    "        # because it probably is not an important word if its not in the vocab\n",
    "        word_vec_matrix.append([0] * vector_dim)\n",
    "\n",
    "print(f\"We have {word_count} words that belong to the pre-trained model out of {num_words}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(np.array(word_vec_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To investigate the clusters, we creat a dictionary that saves the words in that specific cluster and their frequency\n",
    "words_in_cluster = []\n",
    "cluster_labels = list(kmeans.labels_)\n",
    "cluster_centroids = list(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returns a list of tuples of the 10 most similar words of a given list (cosine similarity) to the centroid\n",
    "def get_most_similar(centroid, word_list):\n",
    "    similar_words = model.most_similar(positive=[centroid])\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build cluster words which is a dictionary storing (key, values) = (cluster_id, [words in cluster])\n",
    "# Step 2: Create defaultdict to store words for each cluster\n",
    "cluster_words = defaultdict(list)\n",
    "\n",
    "# Step 3: Retrieve words for each cluster\n",
    "for i, label in enumerate(cluster_labels):\n",
    "    cluster_words[label].append(unique_word_corpus[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. After performing k-means, print the top-10 words for each cluster.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster ID:\n",
      " 4\n",
      "Word:interfaces\t\t\t Similarity: 0.7877712845802307\n",
      "Word:software\t\t\t Similarity: 0.7847545146942139\n",
      "Word:hardware/software\t\t\t Similarity: 0.783154308795929\n",
      "Word:Simulink\t\t\t Similarity: 0.777946412563324\n",
      "Word:multi-threading\t\t\t Similarity: 0.7766555547714233\n",
      "Word:Exadata\t\t\t Similarity: 0.7758769392967224\n",
      "Word:kernel-mode\t\t\t Similarity: 0.7736071348190308\n",
      "Word:server-based\t\t\t Similarity: 0.7725211977958679\n",
      "Word:implementations\t\t\t Similarity: 0.7722378969192505\n",
      "Word:hardware-based\t\t\t Similarity: 0.7716246843338013\n",
      "Cluster ID:\n",
      " 2\n",
      "Word:Miller\t\t\t Similarity: 0.6404972076416016\n",
      "Word:Bennett\t\t\t Similarity: 0.6253702044487\n",
      "Word:Baker\t\t\t Similarity: 0.620003342628479\n",
      "Word:Moore\t\t\t Similarity: 0.6169677376747131\n",
      "Word:McEveety\t\t\t Similarity: 0.6096540093421936\n",
      "Word:Smith\t\t\t Similarity: 0.6080101728439331\n",
      "Word:Robinson\t\t\t Similarity: 0.6079325079917908\n",
      "Word:Thompson\t\t\t Similarity: 0.6040717959403992\n",
      "Word:Walker\t\t\t Similarity: 0.6033617258071899\n",
      "Word:Matthews\t\t\t Similarity: 0.6013290882110596\n",
      "Cluster ID:\n",
      " 1\n",
      "Word:Decision-Making\t\t\t Similarity: 0.7557531595230103\n",
      "Word:Analysis\t\t\t Similarity: 0.716435432434082\n",
      "Word:Linkages\t\t\t Similarity: 0.7099360227584839\n",
      "Word:Structuring\t\t\t Similarity: 0.7083143591880798\n",
      "Word:Evidence-Based\t\t\t Similarity: 0.7018904089927673\n",
      "Word:Processes\t\t\t Similarity: 0.7009482383728027\n",
      "Word:Methodology\t\t\t Similarity: 0.6967429518699646\n",
      "Word:Methodologies\t\t\t Similarity: 0.6948868632316589\n",
      "Word:Measurement\t\t\t Similarity: 0.6933491826057434\n",
      "Word:Applications\t\t\t Similarity: 0.6899436712265015\n",
      "Cluster ID:\n",
      " 0\n",
      "Word:Endocrinology\t\t\t Similarity: 0.2471100091934204\n",
      "Word:fez\t\t\t Similarity: 0.24094511568546295\n",
      "Word:mara\t\t\t Similarity: 0.23802189528942108\n",
      "Word:Tahi\t\t\t Similarity: 0.23225001990795135\n",
      "Word:50-yard\t\t\t Similarity: 0.23101402819156647\n",
      "Word:Minhag\t\t\t Similarity: 0.22933873534202576\n",
      "Word:ken\t\t\t Similarity: 0.22748516499996185\n",
      "Word:Alvania\t\t\t Similarity: 0.2264711558818817\n",
      "Word:second-level\t\t\t Similarity: 0.2260744571685791\n",
      "Word:Rhema\t\t\t Similarity: 0.2240162193775177\n",
      "Cluster ID:\n",
      " 5\n",
      "Word:photoluminescence\t\t\t Similarity: 0.8122641444206238\n",
      "Word:nanocomposite\t\t\t Similarity: 0.7992552518844604\n",
      "Word:electromigration\t\t\t Similarity: 0.7963088154792786\n",
      "Word:time-resolved\t\t\t Similarity: 0.7954248189926147\n",
      "Word:inhomogeneity\t\t\t Similarity: 0.7946292161941528\n",
      "Word:SWNT\t\t\t Similarity: 0.7945781350135803\n",
      "Word:nanostructure\t\t\t Similarity: 0.792590320110321\n",
      "Word:CdSe\t\t\t Similarity: 0.7918816208839417\n",
      "Word:photoemission\t\t\t Similarity: 0.7916472554206848\n",
      "Word:niobate\t\t\t Similarity: 0.7909703254699707\n",
      "Cluster ID:\n",
      " 9\n",
      "Word:gangliosides\t\t\t Similarity: 0.8468369245529175\n",
      "Word:sphingolipids\t\t\t Similarity: 0.8362213373184204\n",
      "Word:sphingolipid\t\t\t Similarity: 0.8356215357780457\n",
      "Word:calcineurin\t\t\t Similarity: 0.8264908790588379\n",
      "Word:PTPkappa\t\t\t Similarity: 0.8254637718200684\n",
      "Word:STAT1\t\t\t Similarity: 0.8221022486686707\n",
      "Word:GLUT4\t\t\t Similarity: 0.8202976584434509\n",
      "Word:GAPDH\t\t\t Similarity: 0.8179380893707275\n",
      "Word:MMP-2\t\t\t Similarity: 0.8179081678390503\n",
      "Word:down-regulation\t\t\t Similarity: 0.8173655271530151\n",
      "Cluster ID:\n",
      " 3\n",
      "Word:integer-valued\t\t\t Similarity: 0.850344181060791\n",
      "Word:asymptotics\t\t\t Similarity: 0.8482409715652466\n",
      "Word:scalar-valued\t\t\t Similarity: 0.8428587913513184\n",
      "Word:vector-valued\t\t\t Similarity: 0.8362740874290466\n",
      "Word:formula_156\t\t\t Similarity: 0.8313423991203308\n",
      "Word:formula_161\t\t\t Similarity: 0.8310178518295288\n",
      "Word:eigenfunction\t\t\t Similarity: 0.8280341029167175\n",
      "Word:formula_127\t\t\t Similarity: 0.8279891610145569\n",
      "Word:generalization\t\t\t Similarity: 0.8264433145523071\n",
      "Word:formula_131\t\t\t Similarity: 0.8258143067359924\n",
      "Cluster ID:\n",
      " 8\n",
      "Word:incentivizing\t\t\t Similarity: 0.6330490708351135\n",
      "Word:implementing\t\t\t Similarity: 0.6285829544067383\n",
      "Word:Overidentification\t\t\t Similarity: 0.6263969540596008\n",
      "Word:sector-specific\t\t\t Similarity: 0.6184201240539551\n",
      "Word:compliance\t\t\t Similarity: 0.616283118724823\n",
      "Word:necessary\t\t\t Similarity: 0.6135095953941345\n",
      "Word:implement\t\t\t Similarity: 0.6103809475898743\n",
      "Word:information-sharing\t\t\t Similarity: 0.6078681349754333\n",
      "Word:employment-related\t\t\t Similarity: 0.6065199971199036\n",
      "Word:e-services\t\t\t Similarity: 0.6059325933456421\n",
      "Cluster ID:\n",
      " 6\n",
      "Word:u-shaped\t\t\t Similarity: 0.6421241164207458\n",
      "Word:Treecreepers\t\t\t Similarity: 0.6390385627746582\n",
      "Word:non-glaucous\t\t\t Similarity: 0.6338358521461487\n",
      "Word:Pronotum\t\t\t Similarity: 0.6315656900405884\n",
      "Word:thickenings\t\t\t Similarity: 0.6313600540161133\n",
      "Word:yellowish-grey\t\t\t Similarity: 0.6305139064788818\n",
      "Word:bristle-like\t\t\t Similarity: 0.6278838515281677\n",
      "Word:corrugations\t\t\t Similarity: 0.6277655959129333\n",
      "Word:rope-like\t\t\t Similarity: 0.6271703839302063\n",
      "Word:extrude\t\t\t Similarity: 0.6271083950996399\n",
      "Cluster ID:\n",
      " 7\n",
      "Word:interrelatedness\t\t\t Similarity: 0.7052280902862549\n",
      "Word:context\t\t\t Similarity: 0.699504017829895\n",
      "Word:emphasizing\t\t\t Similarity: 0.6990487575531006\n",
      "Word:merely\t\t\t Similarity: 0.6987801194190979\n",
      "Word:importantly\t\t\t Similarity: 0.6965306997299194\n",
      "Word:understand\t\t\t Similarity: 0.6948513388633728\n",
      "Word:notion\t\t\t Similarity: 0.693963885307312\n",
      "Word:sense\t\t\t Similarity: 0.6920253038406372\n",
      "Word:contextualization\t\t\t Similarity: 0.6907426118850708\n",
      "Word:inherent\t\t\t Similarity: 0.68943852186203\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cluster_centroids)):\n",
    "    \n",
    "    cluster_id = list(cluster_words.keys())[i]\n",
    "    print(\"Cluster ID:\\n\", cluster_id)\n",
    "    top_10 = get_most_similar(cluster_centroids[cluster_id], cluster_words[i])\n",
    "    \n",
    "    for word, similarity in top_10:\n",
    "        \n",
    "        print(f\"Word:{word}\\t\\t\\t Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What are the different types of clusters that you observe ? Give labels to 10 of the\n",
    "clusters that are representative of the different types that you see.**\n",
    "\n",
    "To find the optimal value for k we simply tried a few values and used our judgement. We did not want k to be too large, nor too small. In the end, we decided with k=10, like in LSI.\n",
    "\n",
    "Here are the lables we give to the clusters:\n",
    "\n",
    "Cluster 0: international terms\n",
    "\n",
    "Cluster 1: methodologies (notice the capital letters)\n",
    "\n",
    "Cluster 2: first/last names\n",
    "\n",
    "Cluster 3: mathematics\n",
    "\n",
    "Cluster 4: computer software/hardware terms\n",
    "\n",
    "Cluster 5: biology\n",
    "\n",
    "Cluster 6: nature/object desription\n",
    "\n",
    "Cluster 7: comprehension\n",
    "\n",
    "Cluster 8: employment\n",
    "\n",
    "Cluster 9: chemistry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Compare the clusters to the topics that you obtained for LSI and LDA.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.13 : Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.14: Document similarity search with outside terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
